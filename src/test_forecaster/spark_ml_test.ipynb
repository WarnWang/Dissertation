{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from regression import CustomRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# conf = SparkConf()\n",
    "# spark = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_df.p')\n",
    "test = pd.read_pickle('test_df.p')\n",
    "\n",
    "# keys = set(train.keys())\n",
    "# keys.remove('000001.SS')\n",
    "# keys.remove('Target')\n",
    "# ass = VectorAssembler(inputCols=list(keys), outputCol=\"features\")\n",
    "# df = spark.createDataFrame(train)\n",
    "# output = ass.transform(df)\n",
    "# test_df = spark.createDataFrame(test)\n",
    "# test_fe = ass.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol='Target', maxIter=10000)\n",
    "model = lr.fit(output)\n",
    "test_re = model.transform(test_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algorithm = CustomRegression(MLPRegressor(hidden_layer_sizes=(16,)))\n",
    "model = algorithm.fit(np.array(output.select('features').rdd.map(lambda x: np.array(list(x)[0])).collect()), \n",
    "                      output.select('Target').rdd.map(lambda x: list(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CustomRegression in module regression.algorithms:\n",
      "\n",
      "class CustomRegression(MassRegressionAlgorithm)\n",
      " |  Custom mass univariate regression algorithm.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  algorithm : object\n",
      " |      An object encapsulating a regression algorithm.\n",
      " |  \n",
      " |      Can be a scikit-learn algorithm or a custom algorithm that adheres to a\n",
      " |      scikit-learn style API. Must implement a fit(X, y) method that takes a\n",
      " |      deisgn matrix and response vector (X and y, both NumPy ndarrays) and\n",
      " |      returns a fitted model. The fitted model should expose data members\n",
      " |      coef_ and intercept_ that contain the fitted coefficients and intercept\n",
      " |      (if any). The fitted model should also implement a score(X, y) method\n",
      " |      giving the r-squared of fit for the given data and a predict(X) method\n",
      " |      that predicts responses given a design matrix.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, algorithm)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from MassRegressionAlgorithm:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit a mass univariate regression model\n",
      " |      \n",
      " |      Using a single design matrix, the same regression model is fit to any\n",
      " |      number of response variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : ndarray, 2d\n",
      " |          The design matrix, a two-dimensional ndarray. Each row is a unique\n",
      " |          measurement and each column is different regressor / explanatory\n",
      " |          variable.\n",
      " |      \n",
      " |      y : array-like (thunder Series, BoltArray, or ndarray)\n",
      " |          Collection of response variables. Can be a thunder Series object,\n",
      " |          where each record is a different set of response variables or a\n",
      " |          local (ndarray) or distributed (BoltArray) with the first dimension\n",
      " |          indexing response varibles and the second dimension indexing data\n",
      " |          points.\n",
      " |  \n",
      " |  fit_and_score(self, X, y)\n",
      " |      Fit a mass univariate regression model and return the scores as well\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : ndarray, 2d\n",
      " |          The design matrix, a two-dimensional ndarray. Each row is a unique\n",
      " |          measurement and each column is different regressor / explanatory\n",
      " |          variable.\n",
      " |      \n",
      " |      y : array-like (thunder Series, BoltArray, or ndarray)\n",
      " |          Collection of response variables. Can be a thunder Series object,\n",
      " |          where each record is a different set of response variables or a\n",
      " |          local (ndarray) or distributed (BoltArray) with the first dimension\n",
      " |          indexing response varibles and the second dimension indexing data\n",
      " |          points.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_df = test_fe.select('features').toPandas()\n",
    "model.predict(test_fe.select('features').rdd.map(lambda x: np.array(list(x)[0])).collect()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = MLPRegressor(hidden_layer_sizes=(16,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, Y = make_regression(n_samples=100, n_features=3, n_informative=3, n_targets=1, noise=1.0)\n",
    "algorithm = CustomRegression(MLPRegressor(hidden_layer_sizes=(16,)))\n",
    "model = algorithm.fit(X, Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = output.select('Target').rdd.map(lambda x: list(x)[0])\n",
    "X = np.array(output.select('features').rdd.map(lambda x: np.array(list(x)[0])).collect())\n",
    "from regression.utils import toseries\n",
    "y = toseries(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.select('Target').rdd.map(lambda x: list(x)[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(16,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toseries(Y).map(lambda v: deepcopy(alg).fit(X, v)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.load('/Users/warn/PycharmProjects/Dissertation/src/stockforecaster/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_num = 26\n",
    "layers = [input_num, input_num / 3 * 2, input_num / 3, 2]\n",
    "mlpc = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol='ChangeDirection',\n",
    "                                      layers=layers,\n",
    "                                      predictionCol='DirPrediction')\n",
    "model = mlpc.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pdf = df.toPandas()\n",
    "# pdf['features'].values\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "pnparray = pdf['features'].values\n",
    "container = np.zeros((pnparray.shape[0], len(pnparray[0])))\n",
    "for i in range(pnparray.shape[0]):\n",
    "    container[i, :] = pnparray[i][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/warn/PycharmProjects/Dissertation/src/stockforecaster/ann_result.p') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "df = spark.read.load('/Users/warn/PycharmProjects/Dissertation/src/stockforecaster/test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(data, columns=['prediction'])\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "# df.withColumn('prediction', getattr(sdf, 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "solver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto'. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "print lr.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\\nfeaturesCol: features column name. (default: features)\\nfitIntercept: whether to fit an intercept term. (default: True)\\nlabelCol: label column name. (default: label)\\nmaxIter: max number of iterations (>= 0). (default: 100)\\npredictionCol: prediction column name. (default: prediction)\\nregParam: regularization parameter (>= 0). (default: 0.0)\\nsolver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto'. (default: auto)\\nstandardization: whether to standardize the training features before fitting the model. (default: True)\\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
